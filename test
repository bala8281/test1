import pandas as pd
from google.cloud import storage

def read_bq_from_gcs(gcs_url, partition, clusters, where_clause, storage_client=None):
    if not gcs_url.startswith('gs://'):
        raise ValueError('GCS URL must start with "gs://".')

    bucket_name, path = gcs_url.split('/', 2)
    if storage_client is None:
        storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(path)

    try:
        # Download the BigQuery table data to a temporary file
        with tempfile.NamedTemporaryFile() as temp_file:
            blob.download_to_filename(temp_file.name)

            # Read the BigQuery table data into a Pandas DataFrame
            df = pd.read_csv(temp_file.name)

            # Filter the DataFrame based on the partition and where clause
            if partition is not None:
                df = df.loc[df['partition'] == partition]

            if where_clause is not None:
                df = df.query(where_clause)
    except Exception as e:
        raise Exception('Failed to read BigQuery table from GCS: {}'.format(e))

    return df

spark = SparkSession.builder.getOrCreate()
df = read_bq_to_spark('gs://my-bucket/my-table.csv', partition='A', clusters=['cluster1', 'cluster2'], where_clause='column1 = 10', spark=spark)

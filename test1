from airflow.providers.google.cloud.operators.dataproc import (
    DataprocCreateClusterOperator,
    DataprocSubmitJobOperator,
)
from airflow.providers.google.cloud.operators.gcs import GCSListObjectsOperator
from airflow.utils.decorators import apply_defaults


class DataprocFlexibleJobOperator(BaseOperator):
    @apply_defaults
    def __init__(
        self,
        *,
        cluster_name,
        job_config,
        cluster_config,
        cluster_type_file_path,
        region=None,
        project_id=None,
        gcp_conn_id='google_cloud_default',
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.cluster_name = cluster_name
        self.job_config = job_config
        self.cluster_config = cluster_config
        self.cluster_type_file_path = cluster_type_file_path
        self.region = region
        self.project_id = project_id
        self.gcp_conn_id = gcp_conn_id

    def execute(self, context):
        cluster_type = GCSListObjectsOperator(
            task_id='read_cluster_type',
            bucket=self.cluster_type_file_path.split('/')[2],
            object=self.cluster_type_file_path.split('/')[3],
        ).execute(context)

        if cluster_type == 'serverless':
            create_cluster_task = None  # No cluster creation for serverless
            submit_job_task = DataprocSubmitJobOperator(
                task_id='submit_serverless_job',
                job=self.job_config,
                region=self.region,
                project_id=self.project_id,
                gcp_conn_id=self.gcp_conn_id,
            )
        else:
            create_cluster_task = DataprocCreateClusterOperator(
                task_id='create_ephemeral_cluster',
                cluster_name=self.cluster_name,
                region=self.region,
                project_id=self.project_id,
                cluster_config=self.cluster_config,
                gcp_conn_id=self.gcp_conn_id,
            )
            submit_job_task = DataprocSubmitJobOperator(
                task_id='submit_job_to_cluster',
                job=self.job_config,
                region=self.region,
                project_id=self.project_id,
                cluster_name=self.cluster_name,
                gcp_conn_id=self.gcp_conn_id,
            )

        if create_cluster_task:
            create_cluster_task >> submit_job_task
            return submit_job_task.execute(context)
        else:
            return submit_job_task.execute(context)


// airflow dag

from airflow import DAG
from airflow.operators.dummy import DummyOperator
from airflow.utils.dates import days_ago

from dags.operators import DataprocFlexibleJobOperator

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': days_ago(2),
    'retries': 1,
}

with DAG(
    'dataproc_scala_spark_job',
    default_args=default_args,
    schedule_interval=None,  # Set a schedule if needed
) as dag:
    start_task = DummyOperator(task_id='start')

    run_scala_spark_job = DataprocFlexibleJobOperator(
        task_id='run_scala_spark_job',
        cluster_name='my-dataproc-cluster',
        job_config={
            'reference': {'project_id': 'your-project-id'},
            'placement': {'cluster_name': 'my-dataproc-cluster'},
            'spark_job': {
                'main_jar
